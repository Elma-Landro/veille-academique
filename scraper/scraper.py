"""
Syst√®me de Veille Acad√©mique - Module Principal de Scraping
===========================================================

Ce module contient les classes et fonctions principales pour le scraping
automatis√© des offres d'emploi acad√©mique sur les diff√©rentes plateformes.

Auteur: Assistant IA Manus
Date: Juin 2025
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import json
import csv
from datetime import datetime, timedelta
import re
from urllib.parse import urljoin, urlparse
import logging
from typing import List, Dict, Optional, Tuple
import os

# Import de notre configuration
try:
    from .config import (
        MAIN_SOURCES, ENGINEERING_SCHOOLS, SPECIALIZED_SOURCES, ACADEMIC_NETWORKS,
        ALL_KEYWORDS, POSITION_TYPES, DEFAULT_HEADERS, REQUEST_DELAYS, MAX_RETRIES,
        REQUIRED_FIELDS
    )
except ImportError:
    from config import (
        MAIN_SOURCES, ENGINEERING_SCHOOLS, SPECIALIZED_SOURCES, ACADEMIC_NETWORKS,
        ALL_KEYWORDS, POSITION_TYPES, DEFAULT_HEADERS, REQUEST_DELAYS, MAX_RETRIES,
        REQUIRED_FIELDS
    )

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('veille_academique.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class JobScraper:
    """
    Classe principale pour le scraping des offres d'emploi acad√©mique.
    
    Cette classe g√®re :
    - Les requ√™tes HTTP avec gestion des erreurs et rate limiting
    - L'extraction et le parsing du contenu HTML
    - Le filtrage des r√©sultats selon les mots-cl√©s
    - La sauvegarde des donn√©es
    """
    
    def __init__(self):
        """Initialise le scraper avec les param√®tres par d√©faut."""
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)
        self.results = []
        self.stats = {
            'total_scraped': 0,
            'total_relevant': 0,
            'sources_processed': 0,
            'errors': 0
        }
        
    def make_request(self, url: str, retries: int = MAX_RETRIES) -> Optional[requests.Response]:
        """
        Effectue une requ√™te HTTP avec gestion des erreurs et rate limiting.
        
        Args:
            url: URL √† requ√™ter
            retries: Nombre de tentatives en cas d'√©chec
            
        Returns:
            Response object ou None en cas d'√©chec
        """
        for attempt in range(retries):
            try:
                # D√©lai al√©atoire pour √©viter la d√©tection de bot
                delay = random.uniform(REQUEST_DELAYS['min_delay'], REQUEST_DELAYS['max_delay'])
                time.sleep(delay)
                
                logger.info(f"Requ√™te vers {url} (tentative {attempt + 1}/{retries})")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                return response
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"Erreur lors de la requ√™te vers {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(REQUEST_DELAYS['error_delay'])
                else:
                    logger.error(f"√âchec d√©finitif pour {url} apr√®s {retries} tentatives")
                    self.stats['errors'] += 1
                    
        return None
    
    def calculate_relevance_score(self, text: str) -> Tuple[float, List[str]]:
        """
        Calcule un score de pertinence bas√© sur la pr√©sence de mots-cl√©s.
        
        Args:
            text: Texte √† analyser (titre + description du poste)
            
        Returns:
            Tuple (score, mots-cl√©s trouv√©s)
        """
        text_lower = text.lower()
        found_keywords = []
        score = 0.0
        
        # Recherche des mots-cl√©s avec pond√©ration
        for keyword in ALL_KEYWORDS:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
                # Pond√©ration selon le type de mot-cl√©
                if keyword.lower() in ['crypto', 'blockchain', 'bitcoin', 'ethereum']:
                    score += 0.3  # Mots-cl√©s tr√®s sp√©cifiques
                elif keyword.lower() in ['gouvernance', 'governance', 'sts']:
                    score += 0.2  # Mots-cl√©s importants
                else:
                    score += 0.1  # Autres mots-cl√©s
        
        # Bonus pour les types de postes recherch√©s
        for position_type in POSITION_TYPES:
            if position_type.lower() in text_lower:
                score += 0.1
                break
                
        return min(score, 1.0), found_keywords
    
    def extract_job_info(self, job_element, source_name: str, base_url: str) -> Optional[Dict]:
        """
        Extrait les informations d'une offre d'emploi depuis un √©l√©ment HTML.
        
        Args:
            job_element: √âl√©ment BeautifulSoup contenant l'offre
            source_name: Nom de la source (pour tra√ßabilit√©)
            base_url: URL de base pour les liens relatifs
            
        Returns:
            Dictionnaire avec les informations extraites ou None
        """
        try:
            # Cette m√©thode sera sp√©cialis√©e pour chaque source
            # Ici on d√©finit la structure g√©n√©rale
            job_info = {
                'source': source_name,
                'found_date': datetime.now().isoformat(),
                'title': '',
                'institution': '',
                'url': '',
                'deadline': '',
                'keywords_match': [],
                'teaching': 'Unknown',
                'language': 'Unknown',
                'location': '',
                'description': '',
                'relevance_score': 0.0
            }
            
            return job_info
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction pour {source_name}: {e}")
            return None
    
    def scrape_academic_positions(self) -> List[Dict]:
        """
        Scrape les offres sur Academic Positions EU.
        
        Returns:
            Liste des offres trouv√©es
        """
        logger.info("D√©but du scraping d'Academic Positions EU")
        jobs = []
        
        # URL de recherche avec filtres pour sciences sociales et Europe
        search_urls = [
            "https://academicpositions.com/jobs/social-sciences",
            "https://academicpositions.com/jobs/humanities", 
            "https://academicpositions.com/jobs/interdisciplinary"
        ]
        
        for search_url in search_urls:
            response = self.make_request(search_url)
            if not response:
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Recherche des √©l√©ments contenant les offres d'emploi
            job_elements = soup.find_all('div', class_=['job-item', 'position-item'])
            
            for job_element in job_elements:
                try:
                    # Extraction du titre
                    title_elem = job_element.find(['h2', 'h3', 'a'], class_=['title', 'job-title'])
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    
                    # Extraction de l'institution
                    institution_elem = job_element.find(['span', 'div'], class_=['institution', 'employer'])
                    institution = institution_elem.get_text(strip=True) if institution_elem else ''
                    
                    # Extraction du lien
                    link_elem = job_element.find('a', href=True)
                    job_url = urljoin(search_url, link_elem['href']) if link_elem else ''
                    
                    # Extraction de la localisation
                    location_elem = job_element.find(['span', 'div'], class_=['location', 'country'])
                    location = location_elem.get_text(strip=True) if location_elem else ''
                    
                    # Calcul de la pertinence
                    full_text = f"{title} {institution} {location}"
                    relevance_score, keywords_found = self.calculate_relevance_score(full_text)
                    
                    # Ne garder que les offres pertinentes
                    if relevance_score > 0.1 or any(keyword.lower() in full_text.lower() for keyword in ALL_KEYWORDS):
                        job_info = {
                            'source': 'Academic Positions EU',
                            'title': title,
                            'institution': institution,
                            'url': job_url,
                            'location': location,
                            'keywords_match': keywords_found,
                            'relevance_score': relevance_score,
                            'found_date': datetime.now().isoformat(),
                            'deadline': '√Ä v√©rifier',
                            'teaching': '√Ä v√©rifier',
                            'language': '√Ä v√©rifier',
                            'description': 'Voir lien pour d√©tails'
                        }
                        jobs.append(job_info)
                        logger.info(f"Offre trouv√©e: {title} - {institution}")
                        
                except Exception as e:
                    logger.error(f"Erreur lors du parsing d'une offre Academic Positions: {e}")
                    continue
        
        logger.info(f"Academic Positions EU: {len(jobs)} offres trouv√©es")
        return jobs
    
    def scrape_jobs_ac_uk(self) -> List[Dict]:
        """
        Scrape les offres sur Jobs.ac.uk.
        
        Returns:
            Liste des offres trouv√©es
        """
        logger.info("D√©but du scraping de Jobs.ac.uk")
        jobs = []
        
        # URLs de recherche pour diff√©rentes disciplines
        search_urls = [
            "https://www.jobs.ac.uk/search/?keywords=social+sciences",
            "https://www.jobs.ac.uk/search/?keywords=digital+humanities",
            "https://www.jobs.ac.uk/search/?keywords=technology+studies"
        ]
        
        for search_url in search_urls:
            response = self.make_request(search_url)
            if not response:
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Recherche des √©l√©ments contenant les offres
            job_elements = soup.find_all(['div', 'article'], class_=['job', 'vacancy', 'result'])
            
            for job_element in job_elements:
                try:
                    # Extraction similaire √† Academic Positions
                    title_elem = job_element.find(['h2', 'h3', 'a'])
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    
                    institution_elem = job_element.find(['span', 'div'], string=re.compile(r'University|College|Institute'))
                    institution = institution_elem.get_text(strip=True) if institution_elem else ''
                    
                    link_elem = job_element.find('a', href=True)
                    job_url = urljoin(search_url, link_elem['href']) if link_elem else ''
                    
                    # Calcul de la pertinence
                    full_text = f"{title} {institution}"
                    relevance_score, keywords_found = self.calculate_relevance_score(full_text)
                    
                    if relevance_score > 0.1:
                        job_info = {
                            'source': 'Jobs.ac.uk',
                            'title': title,
                            'institution': institution,
                            'url': job_url,
                            'keywords_match': keywords_found,
                            'relevance_score': relevance_score,
                            'found_date': datetime.now().isoformat(),
                            'location': 'UK',
                            'deadline': '√Ä v√©rifier',
                            'teaching': '√Ä v√©rifier',
                            'language': 'English',
                            'description': 'Voir lien pour d√©tails'
                        }
                        jobs.append(job_info)
                        
                except Exception as e:
                    logger.error(f"Erreur lors du parsing Jobs.ac.uk: {e}")
                    continue
        
        logger.info(f"Jobs.ac.uk: {len(jobs)} offres trouv√©es")
        return jobs
    
    def run_full_scraping(self) -> Dict:
        """
        Lance le scraping complet sur toutes les sources configur√©es.
        
        Returns:
            Dictionnaire avec les r√©sultats et statistiques
        """
        logger.info("=== D√âBUT DU SCRAPING COMPLET ===")
        start_time = datetime.now()
        
        all_jobs = []
        
        # Scraping des sources principales
        try:
            academic_positions_jobs = self.scrape_academic_positions()
            all_jobs.extend(academic_positions_jobs)
            self.stats['sources_processed'] += 1
        except Exception as e:
            logger.error(f"Erreur Academic Positions: {e}")
            
        try:
            jobs_ac_uk_jobs = self.scrape_jobs_ac_uk()
            all_jobs.extend(jobs_ac_uk_jobs)
            self.stats['sources_processed'] += 1
        except Exception as e:
            logger.error(f"Erreur Jobs.ac.uk: {e}")
        
        # Mise √† jour des statistiques
        self.stats['total_scraped'] = len(all_jobs)
        self.stats['total_relevant'] = len([job for job in all_jobs if job['relevance_score'] > 0.3])
        
        # Tri par score de pertinence
        all_jobs.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # Sauvegarde des r√©sultats
        self.save_results(all_jobs)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"=== SCRAPING TERMIN√â en {duration} ===")
        logger.info(f"Sources trait√©es: {self.stats['sources_processed']}")
        logger.info(f"Offres trouv√©es: {self.stats['total_scraped']}")
        logger.info(f"Offres pertinentes: {self.stats['total_relevant']}")
        logger.info(f"Erreurs: {self.stats['errors']}")
        
        return {
            'jobs': all_jobs,
            'stats': self.stats,
            'duration': str(duration)
        }
    
    def save_results(self, jobs: List[Dict]) -> None:
        """
        Sauvegarde les r√©sultats dans diff√©rents formats.
        
        Args:
            jobs: Liste des offres d'emploi trouv√©es
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde JSON
        json_filename = f"veille_academique_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        logger.info(f"R√©sultats sauvegard√©s en JSON: {json_filename}")
        
        # Sauvegarde CSV
        if jobs:
            csv_filename = f"veille_academique_{timestamp}.csv"
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=REQUIRED_FIELDS)
                writer.writeheader()
                for job in jobs:
                    # S'assurer que tous les champs requis sont pr√©sents
                    row = {field: job.get(field, '') for field in REQUIRED_FIELDS}
                    writer.writerow(row)
            logger.info(f"R√©sultats sauvegard√©s en CSV: {csv_filename}")


def main():
    """Fonction principale pour lancer le scraping."""
    print("üéØ Syst√®me de Veille Acad√©mique")
    print("=" * 50)
    print("Recherche de postes en sciences humaines et sociales")
    print("appliqu√©es aux technologies (crypto, blockchain, STS)")
    print("=" * 50)
    
    # Initialisation du scraper
    scraper = JobScraper()
    
    # Lancement du scraping complet
    results = scraper.run_full_scraping()
    
    # Affichage des r√©sultats
    print(f"\nüìä R√âSULTATS:")
    print(f"   ‚Ä¢ Sources trait√©es: {results['stats']['sources_processed']}")
    print(f"   ‚Ä¢ Offres trouv√©es: {results['stats']['total_scraped']}")
    print(f"   ‚Ä¢ Offres pertinentes: {results['stats']['total_relevant']}")
    print(f"   ‚Ä¢ Dur√©e: {results['duration']}")
    
    if results['jobs']:
        print(f"\nüéØ TOP 5 DES OFFRES LES PLUS PERTINENTES:")
        for i, job in enumerate(results['jobs'][:5], 1):
            print(f"{i}. {job['title']} - {job['institution']}")
            print(f"   Score: {job['relevance_score']:.2f} | Mots-cl√©s: {', '.join(job['keywords_match'][:3])}")
            print(f"   URL: {job['url']}")
            print()


if __name__ == "__main__":
    main()

