
#!/usr/bin/env python3
"""
Script autonome pour le scraping acad√©mique.
Peut √™tre utilis√© avec Replit Scheduled Deployments.
"""

import sys
import os
import requests
import json
from datetime import datetime

# Ajouter le r√©pertoire du scraper au path
sys.path.append(os.path.join(os.path.dirname(__file__), 'scraper'))

def run_scheduled_scraping():
    """Lance un scraping programm√© et envoie les r√©sultats √† l'API."""
    print(f"üöÄ D√©but du scraping programm√© - {datetime.now()}")
    
    try:
        # Import du scraper
        from scraper.scraper import JobScraper
        
        # Lancement du scraping
        scraper = JobScraper()
        results = scraper.run_full_scraping()
        
        print(f"‚úÖ Scraping termin√©: {results['stats']['total_scraped']} offres trouv√©es")
        
        # Optionnel: envoyer les r√©sultats √† l'API backend
        api_url = os.environ.get('API_URL', 'http://localhost:5000/api')
        try:
            response = requests.post(
                f"{api_url}/scraping/results/update",
                json=results['jobs'],
                timeout=30
            )
            if response.status_code == 200:
                print("üì§ R√©sultats envoy√©s √† l'API avec succ√®s")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'envoi √† l'API: {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping: {e}")
        return False

if __name__ == "__main__":
    success = run_scheduled_scraping()
    sys.exit(0 if success else 1)
